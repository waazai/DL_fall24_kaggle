{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "5u7AR_rRwp2N"
      },
      "outputs": [],
      "source": [
        "# %%capture\n",
        "# This cell will take time\n",
        "!pip install unsloth\n",
        "# Also get the latest nightly Unsloth!\n",
        "!pip uninstall unsloth -y && pip install --upgrade --no-cache-dir \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SBC2hctlwwiI"
      },
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "import torch\n",
        "max_seq_length = 2048 # Choose any\n",
        "dtype = None # None for auto detection. Float16 for Tesla T4, V100, Bfloat16 for Ampere+\n",
        "load_in_4bit = True # Use 4bit quantization to reduce memory usage. Can be False.\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fwTeB4jLw4kh"
      },
      "outputs": [],
      "source": [
        "# model path\n",
        "model_path = \"drive/MyDrive/Colab Notebooks/lora_model_2\"\n",
        "\n",
        "# output csv path\n",
        "output_path = 'drive/MyDrive/Colab Notebooks/test2_highr.csv'\n",
        "\n",
        "# the prompt to use\n",
        "prompt = \"\"\"You are a great mathematician and you are tasked with finding if an answer to a given maths question is correct or not. Yout response should be 'True' if correct, otherwise 'False'. Below is Question and Answer.\n",
        "\n",
        "### Question:\n",
        "{}\n",
        "\n",
        "### Answer:\n",
        "{}\n",
        "\n",
        "### Explainaition\n",
        "\n",
        "### Output:\n",
        "{}\"\"\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hXmkWQ5Mx2oF"
      },
      "outputs": [],
      "source": [
        "# load data\n",
        "\n",
        "from datasets import load_dataset\n",
        "dataset = load_dataset(\"ad6398/nyu-dl-teach-maths-comp\")\n",
        "\n",
        "def formatting_prompts_func(examples):\n",
        "    question = examples[\"question\"]\n",
        "    ans       = examples[\"answer\"]\n",
        "    texts = []\n",
        "    for instruction, input in zip(question, ans):\n",
        "        # Must add EOS_TOKEN, otherwise your generation will go on forever!\n",
        "        text = prompt.format(instruction, input, \"\")\n",
        "        texts.append(text)\n",
        "    return { \"text\" : texts, }\n",
        "\n",
        "test_dataset = dataset['test'].map(formatting_prompts_func, batched = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PrGmL-djxMiE"
      },
      "outputs": [],
      "source": [
        "# load model\n",
        "\n",
        "if True:\n",
        "    from unsloth import FastLanguageModel\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name = model_path,\n",
        "        max_seq_length = max_seq_length,\n",
        "        dtype = dtype,\n",
        "        load_in_4bit = load_in_4bit,\n",
        "    )\n",
        "    FastLanguageModel.for_inference(model) # Enable native 2x faster inference\n",
        "\n",
        "\n",
        "# create pipeline for inference\n",
        "\n",
        "from transformers import TextGenerationPipeline\n",
        "class MyPipeline(TextGenerationPipeline):\n",
        "    def postprocess(pipeline, dict):\n",
        "      generated_token = dict['generated_sequence']\n",
        "      input_space_holder = len(dict['input_ids'][0])\n",
        "      is_correct = tokenizer.batch_decode([generated_token[0][0][input_space_holder:]], skip_special_tokens=True)\n",
        "      return is_correct[0]\n",
        "\n",
        "pipe = MyPipeline(\n",
        "    task=\"text-generation\",\n",
        "    model = model,\n",
        "    tokenizer = tokenizer,\n",
        "    batch_size = 4,\n",
        "    eos_token_id = model.config.eos_token_id,\n",
        "    max_new_tokens = 64\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "aP_ALkn5xzoR"
      },
      "outputs": [],
      "source": [
        "# inference\n",
        "\n",
        "with open(output_path, 'w') as f:\n",
        "  f.write('ID,is_correct\\n')\n",
        "\n",
        "id = 0\n",
        "for i in range(10):\n",
        "  # sperate into 10 runs\n",
        "  l = 1000*i  # set to a small number to test run\n",
        "  r = 1000*(i+1)  # set to a small number to test run\n",
        "  print(f'Running on data {l} to {r}...')\n",
        "  res = pipe(test_dataset['text'][l:r])\n",
        "  with open('output_path', 'a') as f:\n",
        "    for i in res:\n",
        "      f.write(f'{id},{i}\\n')\n",
        "      id += 1\n",
        "  print(f'Data {l} to {r} written to file.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(output_path)\n",
        "df['is_correct'] = df['is_correct'].map({'False': False, 'True': True})\n",
        "df['ID'] = df['ID'].map(lambda x: int(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "type(df['is_correct'][0])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
